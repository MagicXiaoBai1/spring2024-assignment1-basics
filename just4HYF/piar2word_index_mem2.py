
import mmap
import regex as re
from collections import defaultdict

# GPT-2 åˆ†è¯æ­£åˆ™ï¼ˆä¸å®˜æ–¹ä¸€è‡´ï¼‰
GPT2_REGEX = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""

def utf8_byte_encode(text: str):
    """å°†å­—ç¬¦ä¸²è½¬ä¸º GPT-2 é£æ ¼çš„å­—èŠ‚ token åˆ—è¡¨ + </w>"""
    return tuple([f"b{b}" for b in text.encode("utf-8")] + ["</w>"])

def get_pairs(word):
    """è¿”å›ç›¸é‚»ç¬¦å·å¯¹é›†åˆ"""
    return set(zip(word, word[1:]))

def is_chunk_complete_utf8(text: bytes):
    """æ£€æŸ¥æ–‡æœ¬å—æ˜¯å¦ä¸ºå®Œæ•´çš„ UTF-8 ç¼–ç """
    try:
        text.decode("utf-8")
        return True
    except UnicodeDecodeError:
        return False
complete_end = end
while offset < complete_end and not is_chunk_complete_utf8(mm[offset:end]):
    complete_end -= 1

chunk_text = mm[offset:complete_end].decode("utf-8", errors="ignore")
def train_bpe_from_large_file(filepath, min_freq=2):
    """
    ç›´æ¥ä»è¶…å¤§æ–‡æœ¬æ–‡ä»¶è®­ç»ƒ BPE åˆå§‹ pair é¢‘æ¬¡ï¼ˆä¸å­˜ tokensï¼‰
    è¿”å›: pair_freqs: dict[(str,str), int]
    """
    pair_freqs = defaultdict(int)
    buffer = ""  # ç”¨äºæ‹¼æ¥è·¨å—çš„æ®‹ç‰‡

    with open(filepath, "rb") as f:  # æ³¨æ„ï¼šmmap éœ€è¦äºŒè¿›åˆ¶æ¨¡å¼
        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
            file_size = len(mm)
            offset = 0
            chunk_size = 64 * 1024 * 1024  # 64MB chunks

            while offset < file_size:
                # è¯»å–ä¸€ä¸ª chunkï¼ˆbytesï¼‰
                end = min(offset + chunk_size, file_size)
                chunk_bytes = mm[offset:end]

                # å°è¯•å®Œæ•´è§£ç  UTF-8ï¼›è‹¥å¤±è´¥ï¼Œå›é€€åˆ°æœ€åä¸€ä¸ªå®Œæ•´å­—ç¬¦
                length = len(chunk_bytes)
                complete_end = end
                while offset < complete_end and not is_chunk_complete_utf8(mm[offset:end]):
                    complete_end -= 1

                chunk_text = mm[offset:complete_end].decode("utf-8", errors="ignore")
                # æ‹¼æ¥ä¸Šä¸€è½®æ®‹ç‰‡
                text = buffer + chunk_text

                # å¦‚æœä¸æ˜¯æœ€åä¸€å—ï¼Œä¿ç•™æœ«å°¾å¯èƒ½ä¸å®Œæ•´çš„ token
                if offset < file_size:
                    # æ‰¾åˆ°æœ€åä¸€ä¸ªç©ºç™½ç¬¦ï¼ˆå®‰å…¨æ–­ç‚¹ï¼‰
                    safe_pos = -1
                    for i in range(len(text) - 1, -1, -1):
                        if text[i] in (' ', '\n', '\t', '\r'):
                            safe_pos = i
                            break
                    if safe_pos != -1:
                        process_text = text[:safe_pos + 1]
                        buffer = text[safe_pos + 1:]
                    else:
                        # æ²¡æœ‰å®‰å…¨æ–­ç‚¹ï¼Ÿæš‚æ—¶å…¨å¤„ç†ï¼ˆé£é™©ä½ï¼Œå›  chunk å¤§ï¼‰
                        process_text = text
                        buffer = ""
                else:
                    # æœ€åä¸€å—ï¼Œå¤„ç†å…¨éƒ¨
                    process_text = text
                    buffer = ""

                # ğŸ”¥ æ ¸å¿ƒï¼šæµå¼æ­£åˆ™åŒ¹é… + BPE åˆå§‹åŒ–
                for match in re.finditer(GPT2_REGEX, process_text):
                    token = match.group()
                    if not token.strip():
                        continue  # è·³è¿‡çº¯ç©ºç™½ï¼ˆå¯é€‰ï¼‰

                    # è½¬ä¸º BPE åˆå§‹ç¬¦å·åºåˆ—ï¼ˆå­—èŠ‚çº§ï¼‰
                    symbols = utf8_byte_encode(token)
                    # ç»Ÿè®¡æ‰€æœ‰ç›¸é‚» pair
                    for pair in get_pairs(symbols):
                        pair_freqs[pair] += 1

            # å¤„ç†æœ€åæ®‹ç‰‡
            if buffer:
                for match in re.finditer(GPT2_REGEX, buffer):
                    token = match.group()
                    if not token.strip():
                        continue
                    symbols = utf8_byte_encode(token)
                    for pair in get_pairs(symbols):
                        pair_freqs[pair] += 1

    # è¿‡æ»¤ä½é¢‘ pairï¼ˆå¯é€‰ï¼‰
    if min_freq > 1:
        pair_freqs = {k: v for k, v in pair_freqs.items() if v >= min_freq}

    return pair_freqs



